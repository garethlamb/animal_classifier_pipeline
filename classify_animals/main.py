import json
import pickle
import os
import math
import logging
import shutil
import torch
import numpy as np
import pandas as pd
from textwrap import dedent
from tensorflow import keras as kr
from tqdm import tqdm
from classify_animals.scripts import config, crop_images
from classify_animals.keras_scripts.keras_identify_species import keras_identify_species
from classify_animals.pytorch_scripts.pytorch_identify_species import pytorch_identify_species
from classify_animals.pytorch_scripts.engine import Engine, load_pytorch_model
from os.path import join as jpth

def classify_animals(
    bb_results_path, 
    image_dir, 
    model_path, 
    working_data_dir = None, 
    keep_crops: bool = True ,
    md_thr = 0.2, 
    ent_thr = None, 
    batch_size: int = 32 , 
    classifier_batch_size: int = None , 
    use_checkpoints: bool = True ,
    show_progress_bar: bool = True
):
    """
    Crops animal detections generated by Megadetector and sorts crops
    by species of animal
    
    bb_results_path (String or Python PATH object): Path to 
        Megadetector's JSON string output for bounding boxes. 
        Can either by a Pyrhon PATH object, absolute or 
        relative path
    
    image_dir (String or Python PATH object): Path to directory
        that contains raw camera trap images. This path must
        be the same as the one that was used to generate the 
        megadetector bounding boxes. That is to say, the relative
        file paths in the JSON string outputted by megadetector must
        be relative to the path given by image_dir.
    
    model_path (String or Python PATH object) : Path to pre-trained
        classifier. If path leads to a file then it will assume
        that it's a keras H5 model, otherwise it will assume that
        it's a PyTorch model that was saved using the same format
        as this approach that uses active learning:
        
    
    working_data_dir (String or Python PATH object) : Path
        to directory that will contain the temporary files
        the program needs to use to run and the output of
        the program. if set to None, it will default to saving
        the output to a folder that lies in the working directory
        
    keep_crops (Bool) : If True, Megadetector's animal detections
        will be cropped and saved to the working data_dir. Furthermore,
        the crops will be sorted by the species that the neural network
        detected them to be
        
    md_thr (float) : Threshold value for the confidence ratings of
        Megadetector's detections. Only bounding boxes with a confidence
        rating above the threshold will be cropped and analysed. A higher
        threshold means that detections are more likely to be of an animal
        but it also makes it more likely that fewer animals will be detected
        
    ent_thr (float) : Threshold value for confidence rating in neural network's
        classifications. Confidence rating is Shannon Entropy of the network's
        output and therefore only classifications that have an entropy below the 
        threshold will be assigned to their crops. A lower threshold will increase
        the precision of the classifier however it also means that more animal
        detections will be labelled as having an unknown species. If an entropy
        threshold is used then a cut-off of 1.0 might be a good value to start at
        although this hasn't been properly tested.
        
    batch_size (int) : Number of images to process at a time. Lowering this number
        should reduce the program's requirement for memory and storage space. Raising
        this should will likely reduce the overall time it takes for the neural network
        to process the images
    
    classifier_batch_size (int) : Batch size for images as they're processed by the
        neural network classifier. By default, its half of the batch size of the cropping
        algorithm rounded up to the nearest integer. Classifier_batch_size should be at
        most batch_size
    
    use_checkpoints (Bool): If True, the program will start from check points found
        in working_data_dir, which is useful if program stops during processing. If
        False, the program will start the whole process from scratch and overwrite
        checkpoints
        
    show_progress_bar (Bool): If True, a progress bar will be displayed
    """
    
    # Get type of model
    model_file_extension = os.path.splitext(model_path)[1].lower()
    
    # Validate model files
    if model_file_extension in ['.h5', '.keras']: model_type = 'keras'
    elif model_file_extension == '': 
        model_type = 'pytorch'
        
        if ent_thr is not None:
            logging.warning(dedent(
                """
                Entropy threshold will be ignored as model was
                detected as a PyTorch model. If model is a keras
                H5 model then please ensure that the file extension
                is either '.h5' or '.keras'.
                """
            ))
        
        assert os.path.isfile(jpth(model_path, config.CLASSIFIER_NAME)), 'PyTorch classifier {} could not be found in model folder'.format(config.CLASSIFIER_NAME)
        assert os.path.isfile(jpth(model_path, config.EMBEDDING_MODEL_NAME)), 'PyTorch embedding model {} could not be found in model folder'.format(config.EMBEDDING_MODEL_NAME)
        assert os.path.isfile(jpth(model_path, config.EMBEDDING_MEAN_NAME)), 'PyTorch fitted dataset mean {} could not be found in model folder'.format(config.EMBEDDING_MEAN_NAME)
        assert os.path.isfile(jpth(model_path, config.EMBEDDING_STD_NAME)), 'PyTorch fitted dataset standard deviation {} could not be found in model folder'.format(config.EMBEDDING_STD_NAME)  
        
        # Check if a GPU is available
        device = "cuda" 
        if not torch.cuda.is_available():
            device = "cpu"
    
    else: raise ValueError(dedent(
        """
        Model file type could not be recognised. If the model was built
        with keras, please ensure that it has either an '.h5' or '.keras'
        file extension. If the model was built in PyTorch then please
        ensure that the model_path points to the directory that contains
        the PyTorch files
        """
    ))
    
        
        # Raise error if not all Pytorch files can be found
        
    # Set nn batch size to default value if none was passed
    if classifier_batch_size == None:
        classifier_batch_size = math.ceil(batch_size / 2)
    
    # If no working directory was passed to the program
    if working_data_dir == None:
        
        # Set default working directory
        working_data_dir = jpth(os.getcwd(), config.WORK_DIR_DEFAULT_NAME) 
    
    # Build dictionary of file paths used by the program
    path_dict = config.build_working_file_dict(working_data_dir)
    
    # If checkpoints exist and should be used
    if use_checkpoints and config.all_checkpoints_exist(path_dict):
        
        # Load Checkpoints
        df_bb = pd.read_csv(path_dict['bb_csv_path'])
        with open(path_dict['crop_checkpoint_path'], 'rb') as file:
            crops = pickle.load(file)
        with open(path_dict['batch_number_path'], 'r') as file:
            start_row_number = json.load(file)
        
    else:
        
        # Otherwise, create data structures for batch-classifying
        # data from scratch
        df_bb, crops, start_row_number = config.initialise_checkpoints(bb_results_path, md_thr)
        
        # Save CSV on bounding box information
        config.save_csv(df_bb, path_dict['bb_csv_path'], save_index = False)
        
        # Save checkpoint
        config.save_checkpoint(crops, start_row_number, path_dict)
    
    
    # Load trained classifier
    if model_type == 'keras':
        model = kr.models.load_model(model_path)
    elif model_type == 'pytorch':
        embedding_model, classifier, feat_dim = load_pytorch_model(
            embedding_model_path = jpth(model_path, config.EMBEDDING_MODEL_NAME),
            classifier_path = jpth(model_path, config.CLASSIFIER_NAME),
            device = device
        )
        eng = Engine(
            embedding_model = embedding_model,
            classifier = classifier,
            feat_dim = feat_dim,
            device = device
        )
        
        # Load fitted image pixel means and standard deviations
        mean = np.load(
            file = jpth(model_path, config.EMBEDDING_MEAN_NAME)
        )
        std = np.load(
            file = jpth(model_path, config.EMBEDDING_STD_NAME)
        )
    else:
        raise NotImplementedError(dedent(
            """
            Could not load this type of model
            as it has not been implemented yet.
            """
        ))
    
    # Process images in batches
    for i in tqdm(range(start_row_number, df_bb.shape[0], batch_size), disable = not show_progress_bar):
    
        # Get bounding boxes for batch
        df_bb_batch = df_bb.iloc[i : i + batch_size].copy() 
        
        # Get folder that crops from batch will be put into
        batch_subfolder = 'crops_' + str(i) 
        
        # Crop images
        df_crop_batch = crop_images.main(
            df_bb = df_bb_batch, 
            source_dir = image_dir, 
            target_dir = path_dict['unsorted_cropped_images_dir'],
            subfolder_name = batch_subfolder
        )
        
        # Identify Species
        if model_type == 'keras':
            df_crop_batch = keras_identify_species(
                df_crop = df_crop_batch, 
                root_dir = path_dict['unsorted_cropped_images_dir'],
                img_dir = jpth(path_dict['unsorted_cropped_images_dir'], batch_subfolder), 
                model = model, 
                batch_size = classifier_batch_size, 
                ent_thr = ent_thr
            )
        elif model_type == 'pytorch':
            df_crop_batch = pytorch_identify_species(
                df_img = df_crop_batch, 
                root_dir = path_dict['unsorted_cropped_images_dir'],
                engine = eng,
                mean = mean,
                std = std,
                batch_size = classifier_batch_size
            )
        else:
            raise NotImplementedError(dedent(
                """
                Could not identify species with this type
                of model as it has not been implemented yet.
                """
            ))
        
        # If crops should not be saved
        if not keep_crops: 
            
            # Delete cropped image files for this batch
            df_crop_batch[config.CROP_COL_NAME].apply(
                lambda x : os.remove(jpth(path_dict['unsorted_cropped_images_dir'], x))
            )
            
            # Delete batch subfolder
            os.rmdir(jpth(path_dict['unsorted_cropped_images_dir'], batch_subfolder))
            
            # Delete crop file paths from batch
            df_crop_batch.drop(columns = config.CROP_COL_NAME, inplace = True)
        
        # Add crops to list of crops
        crops.append(df_crop_batch)
        
        # Save checkpoint
        config.save_checkpoint(crops, i + batch_size, path_dict)
        
        # Log progress
        logging.info(f'Processed {i + batch_size} crops')
    
    
    # Clear classifier from memory
    if model_type == 'keras': kr.backend.clear_session() 
    
    # Compile animal labels
    df_crop = pd.concat(crops, ignore_index = True)
    
    # If crops should be saved
    if keep_crops: 
        
        # Sort cropped files by species
        df_crop = df_crop.groupby(
            config.SPECIES_COL_NAME, 
            group_keys = False
        ).apply(
            lambda x : config.sort_by_species(
                df = x, 
                old_dir = path_dict['unsorted_cropped_images_dir'], 
                new_dir = path_dict['sorted_cropped_images_dir'], 
                species = x.name, 
                path_col_name = config.CROP_COL_NAME
            )
        )
        
        # Delete unsorted_cropped_images folder
        config.delete_folder_if_empty(path_dict['unsorted_cropped_images_dir'])
    
    # Save final animal labels
    config.save_csv(df_crop, path_dict['output_path'], save_index = False)
    
    # Delete checkpoints
    shutil.rmtree(path_dict['checkpoint_dir'])